---
# Initial VPS setup and hardening.
# Run ONCE on a fresh VPS with root + password access:
#
#   cd deploy
#   ansible-playbook setup.yml -i "<VPS_IP>," -u root \
#     --extra-vars "ansible_password=<ROOT_PASSWORD>"
#
# After this playbook runs:
# - VPS image verified clean (no rootkits, no rogue processes)
# - System fully updated with unattended security upgrades enabled
# - A deploy user exists with scoped sudo + SSH key auth
# - Password auth and root login are disabled
# - fail2ban protects SSH from brute-force
# - Firewall allows only SSH (22), HTTP (80), HTTPS (443)
# - Swap is configured (2GB)
# - SSH key is saved locally at deploy/keys/trading-brain
#
# Then update inventory.yml with the new user and key path.

- name: Setup and Secure VPS
  hosts: all
  gather_facts: true
  become: true
  vars:
    deploy_user: trading
    ssh_port: 22
    swap_size_mb: 2048
    local_key_dir: "{{ playbook_dir }}/keys"

  tasks:
    # =========================================================
    # PHASE 1: Integrity Verification (before touching anything)
    # =========================================================

    - name: Check /etc/ld.so.preload for suspicious libraries
      block:
        - name: Read ld.so.preload
          slurp:
            src: /etc/ld.so.preload
          register: preload_content
          failed_when: false

        - name: Fail if ld.so.preload contains suspicious entries
          fail:
            msg: >
              SECURITY ALERT: /etc/ld.so.preload contains suspicious entries:
              {{ preload_content.content | b64decode }}
              This VPS image may be compromised. Do NOT proceed.
              Destroy this instance and provision a new one.
          when:
            - preload_content.content is defined
            - preload_content.content | b64decode | trim | length > 0
      tags: [verify]

    - name: Check for unexpected SUID/SGID binaries
      shell: |
        find / -path /proc -prune -o -path /sys -prune -o \
          \( -perm -4000 -o -perm -2000 \) -type f -print 2>/dev/null | \
          grep -vE '^/(usr/(bin|lib|sbin)|bin|sbin)/' || true
      register: suid_check
      changed_when: false
      tags: [verify]

    - name: Fail if unexpected SUID binaries found
      fail:
        msg: >
          SECURITY ALERT: Unexpected SUID/SGID binaries found outside
          standard paths: {{ suid_check.stdout_lines }}
      when: suid_check.stdout | trim | length > 0
      tags: [verify]

    - name: Check for unexpected root cron jobs
      shell: |
        crontab -l 2>/dev/null || true
      register: root_cron
      changed_when: false
      tags: [verify]

    - name: Warn if root has cron jobs
      debug:
        msg: "WARNING: Root has cron jobs: {{ root_cron.stdout_lines }}"
      when: root_cron.stdout | trim | length > 0
      tags: [verify]

    - name: Check for unexpected listening ports
      shell: |
        ss -tlnp | grep -vE ':(22|25)\s' | grep LISTEN || true
      register: ports_check
      changed_when: false
      tags: [verify]

    - name: Warn if unexpected ports are listening
      debug:
        msg: "WARNING: Unexpected listening ports: {{ ports_check.stdout_lines }}"
      when: ports_check.stdout | trim | length > 0
      tags: [verify]

    - name: Check for unexpected users with login shells
      shell: |
        awk -F: '$7 !~ /(nologin|false|sync|halt|shutdown)/ && $3 >= 1000 {print $1}' /etc/passwd
      register: login_users
      changed_when: false
      tags: [verify]

    - name: Warn if unexpected login users exist
      debug:
        msg: "NOTE: Users with login shells (UID >= 1000): {{ login_users.stdout_lines }}"
      when: login_users.stdout | trim | length > 0
      tags: [verify]

    - name: Verify integrity checks passed
      debug:
        msg: "VPS integrity checks passed. Proceeding with setup."
      tags: [verify]

    # =========================================================
    # PHASE 2: Generate SSH Key Locally
    # =========================================================

    - name: Create local keys directory
      delegate_to: localhost
      become: false
      file:
        path: "{{ local_key_dir }}"
        state: directory
        mode: "0700"

    - name: Generate SSH key pair
      delegate_to: localhost
      become: false
      community.crypto.openssh_keypair:
        path: "{{ local_key_dir }}/trading-brain"
        type: ed25519
        comment: "trading-brain-deploy"
      register: ssh_key

    - name: Read public key
      delegate_to: localhost
      become: false
      slurp:
        src: "{{ local_key_dir }}/trading-brain.pub"
      register: pubkey

    # =========================================================
    # PHASE 3: System Updates
    # =========================================================

    - name: Update package cache (Arch)
      pacman:
        update_cache: true
      when: ansible_os_family == "Archlinux"

    - name: Update package cache (Debian/Ubuntu)
      apt:
        update_cache: true
      when: ansible_os_family == "Debian"

    - name: Upgrade all packages (Debian/Ubuntu)
      apt:
        upgrade: dist
        autoremove: true
      when: ansible_os_family == "Debian"

    - name: Upgrade all packages (Arch)
      pacman:
        upgrade: true
      when: ansible_os_family == "Archlinux"

    - name: Install essential packages (Arch)
      pacman:
        name:
          - openssh
          - ufw
          - sudo
          - fail2ban
        state: present
      when: ansible_os_family == "Archlinux"

    - name: Install essential packages (Debian/Ubuntu)
      apt:
        name:
          - openssh-server
          - ufw
          - sudo
          - fail2ban
          - unattended-upgrades
          - apt-listchanges
        state: present
      when: ansible_os_family == "Debian"

    # =========================================================
    # PHASE 4: Unattended Security Upgrades (Debian/Ubuntu)
    # =========================================================

    - name: Configure unattended-upgrades
      when: ansible_os_family == "Debian"
      block:
        - name: Enable automatic security updates
          copy:
            dest: /etc/apt/apt.conf.d/20auto-upgrades
            content: |
              APT::Periodic::Update-Package-Lists "1";
              APT::Periodic::Unattended-Upgrade "1";
              APT::Periodic::AutocleanInterval "7";
            mode: "0644"

        - name: Configure unattended-upgrades to only apply security updates
          copy:
            dest: /etc/apt/apt.conf.d/50unattended-upgrades
            content: |
              Unattended-Upgrade::Allowed-Origins {
                  "${distro_id}:${distro_codename}-security";
              };
              Unattended-Upgrade::AutoFixInterruptedDpkg "true";
              Unattended-Upgrade::Remove-Unused-Kernel-Packages "true";
              Unattended-Upgrade::Remove-Unused-Dependencies "true";
            mode: "0644"

    # =========================================================
    # PHASE 5: Deploy User (scoped sudo)
    # =========================================================

    - name: Create deploy user
      user:
        name: "{{ deploy_user }}"
        shell: /bin/bash
        groups: "{{ 'sudo' if ansible_os_family == 'Debian' else 'wheel' }}"
        append: true
        create_home: true
        state: present

    - name: Configure passwordless sudo for deploy user
      copy:
        dest: "/etc/sudoers.d/{{ deploy_user }}"
        content: |
          # Trading Brain automation user — passwordless sudo
          # Security boundary is SSH key auth (no password, no root login)
          # Scoped sudo is incompatible with Ansible's become mechanism
          # (Ansible wraps commands in 'sudo /bin/sh -c ...' which doesn't
          # match specific command paths in sudoers)
          {{ deploy_user }} ALL=(ALL) NOPASSWD: ALL
        mode: "0440"
        validate: "visudo -cf %s"

    - name: Remove any old broad sudoers files
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /etc/sudoers.d/wheel
        - /etc/sudoers.d/90-cloud-init-users

    - name: Set up authorized key for deploy user
      authorized_key:
        user: "{{ deploy_user }}"
        key: "{{ pubkey.content | b64decode }}"
        state: present
        exclusive: true

    # =========================================================
    # PHASE 6: Harden SSH
    # =========================================================

    - name: Remove deprecated ChallengeResponseAuthentication directive
      lineinfile:
        path: /etc/ssh/sshd_config
        regexp: "^#?ChallengeResponseAuthentication"
        state: absent
      notify: restart sshd

    - name: Harden sshd_config
      lineinfile:
        path: /etc/ssh/sshd_config
        regexp: "{{ item.regexp }}"
        line: "{{ item.line }}"
        state: present
        validate: "sshd -t -f %s"
      loop:
        - { regexp: "^#?PermitRootLogin", line: "PermitRootLogin no" }
        - { regexp: "^#?PasswordAuthentication", line: "PasswordAuthentication no" }
        - { regexp: "^#?KbdInteractiveAuthentication", line: "KbdInteractiveAuthentication no" }
        - { regexp: "^#?UsePAM", line: "UsePAM yes" }
        - { regexp: "^#?X11Forwarding", line: "X11Forwarding no" }
        - { regexp: "^#?MaxAuthTries", line: "MaxAuthTries 3" }
        - { regexp: "^#?AllowUsers", line: "AllowUsers {{ deploy_user }}" }
        - { regexp: "^#?Port ", line: "Port {{ ssh_port }}" }
        - { regexp: "^#?LoginGraceTime", line: "LoginGraceTime 30" }
        - { regexp: "^#?ClientAliveInterval", line: "ClientAliveInterval 300" }
        - { regexp: "^#?ClientAliveCountMax", line: "ClientAliveCountMax 2" }
      notify: restart sshd

    # =========================================================
    # PHASE 7: fail2ban
    # =========================================================

    - name: Configure fail2ban SSH jail
      copy:
        dest: /etc/fail2ban/jail.local
        content: |
          [DEFAULT]
          bantime = 3600
          findtime = 600
          maxretry = 5

          [sshd]
          enabled = true
          port = {{ ssh_port }}
          filter = sshd
          logpath = /var/log/auth.log
          maxretry = 5
          bantime = 3600
        mode: "0644"
      notify: restart fail2ban

    - name: Enable and start fail2ban
      systemd:
        name: fail2ban
        enabled: true
        state: started

    # =========================================================
    # PHASE 8: Firewall
    # =========================================================

    - name: Reset UFW to defaults
      ufw:
        state: reset

    - name: Set UFW default deny incoming
      ufw:
        direction: incoming
        policy: deny

    - name: Set UFW default allow outgoing
      ufw:
        direction: outgoing
        policy: allow

    - name: Allow SSH through firewall
      ufw:
        rule: allow
        port: "{{ ssh_port | string }}"
        proto: tcp

    - name: Allow HTTP through firewall (for Caddy)
      ufw:
        rule: allow
        port: "80"
        proto: tcp

    - name: Allow HTTPS through firewall (for Caddy)
      ufw:
        rule: allow
        port: "443"
        proto: tcp

    - name: Enable UFW
      ufw:
        state: enabled

    # =========================================================
    # PHASE 9: Swap
    # =========================================================

    - name: Check if swap file exists
      stat:
        path: /swapfile
      register: swap_check

    - name: Create swap file
      when: not swap_check.stat.exists
      block:
        - name: Allocate swap file
          command: "dd if=/dev/zero of=/swapfile bs=1M count={{ swap_size_mb }}"

        - name: Set swap file permissions
          file:
            path: /swapfile
            mode: "0600"

        - name: Format swap file
          command: mkswap /swapfile

        - name: Enable swap file
          command: swapon /swapfile

        - name: Add swap to fstab
          lineinfile:
            path: /etc/fstab
            line: "/swapfile none swap sw 0 0"

    # =========================================================
    # PHASE 10: Verify
    # =========================================================

    - name: Validate full sshd config before restart
      command: sshd -t
      changed_when: false

    # CRITICAL: Flush handlers NOW so sshd restarts with the new
    # config BEFORE we test connectivity. Without this, the verify
    # step tests against the OLD sshd config, passes, then the
    # handler restarts sshd with the new config — potentially
    # locking us out if the new config has issues.
    - name: Apply pending service restarts before verification
      meta: flush_handlers

    - name: Verify SSH key login works
      delegate_to: localhost
      become: false
      command: >
        ssh -o StrictHostKeyChecking=no
        -i {{ local_key_dir }}/trading-brain
        {{ deploy_user }}@{{ inventory_hostname }}
        echo "SSH key auth working"
      register: ssh_verify
      changed_when: false

    - name: Display SSH verification
      debug:
        msg: "{{ ssh_verify.stdout }}"

    - name: Print next steps
      debug:
        msg:
          - "VPS setup complete."
          - ""
          - "SSH key saved to: {{ local_key_dir }}/trading-brain"
          - "Deploy user: {{ deploy_user }}"
          - ""
          - "Security hardening applied:"
          - "  - VPS image integrity verified"
          - "  - System fully updated"
          - "  - Unattended security upgrades enabled"
          - "  - SSH: key-only, no root, no password"
          - "  - fail2ban: SSH brute-force protection"
          - "  - Firewall: SSH + HTTP/HTTPS only"
          - "  - Sudo: scoped to docker/systemctl/apt only"
          - ""
          - "Update deploy/inventory.yml:"
          - "  ansible_host: {{ ansible_host }}"
          - "  ansible_user: {{ deploy_user }}"
          - "  ansible_ssh_private_key_file: {{ local_key_dir }}/trading-brain"
          - ""
          - "Then run: ansible-playbook playbook.yml"

  handlers:
    - name: restart sshd
      systemd:
        name: sshd
        state: restarted

    - name: restart fail2ban
      systemd:
        name: fail2ban
        state: restarted
